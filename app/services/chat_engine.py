"""
Chat Engine for Text-to-SQL functionality.
Processes natural language questions and converts them to SQL queries.
"""

import json
import re
import asyncio
import sqlite3
from pathlib import Path
from typing import Optional
from datetime import datetime

from app.services.llm_client import call_llm, call_llm_raw_async
from app.services.chat_prompts import build_system_prompt, get_schema_summary


# Database path for direct SQLite access (read-only)
DB_PATH = Path(__file__).parent.parent.parent / "violation_tracking.db"


def determine_complexity(question: str) -> str:
    """
    Simple complexity router based on question characteristics.
    Returns 'flash' for simple queries, 'pro' for complex ones.
    """
    question_lower = question.lower()
    
    # Complex indicators
    complex_patterns = [
        'compare', 'correlation', 'trend', 'over time', 'between',
        'group by', 'having', 'subquery', 'nested', 'join',
        'percentage', 'ratio', 'rate', 'average per'
    ]
    
    for pattern in complex_patterns:
        if pattern in question_lower:
            return 'pro'
    
    # Default to flash for simple queries
    return 'flash'


def execute_query(sql_query: str) -> tuple:
    """
    Executes a SQL query against the VioTrack database.
    Enforces Read-Only access by blocking harmful commands.
    
    Returns:
        tuple: (results_list, columns_list, error_message)
    """
    forbidden_keywords = ["DROP", "DELETE", "INSERT", "UPDATE", "ALTER", "CREATE"]
    
    query_upper = sql_query.upper()
    for keyword in forbidden_keywords:
        if keyword in query_upper:
            return None, None, f"Query contains forbidden keyword '{keyword}'. Read-only access only."
    
    try:
        conn = sqlite3.connect(str(DB_PATH))
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        cursor.execute(sql_query)
        rows = cursor.fetchall()
        
        if rows:
            columns = list(rows[0].keys())
            results = [dict(row) for row in rows]
        else:
            columns = []
            results = []
        
        conn.close()
        return results, columns, None
        
    except Exception as e:
        return None, None, str(e)


def _retry_with_pro(user_question: str, full_prompt: str, 
                    previous_error: str = None, previous_sql: str = None,
                    attempt_number: int = 2, max_retries: int = 3) -> tuple:
    """
    Retry with PRO (Sonnet) model. Supports multiple retry attempts.
    """
    print(f"[CHAT RETRY {attempt_number}] Using PRO model...")
    
    if previous_error and previous_sql:
        retry_prompt = f"""
        Attempt {attempt_number - 1} failed with error: {previous_error}
        The query was: {previous_sql}
        
        Please carefully analyze the error and correct the SQL query to answer: "{user_question}"
        
        IMPORTANT: Make sure to:
        - Check table and column names match the schema exactly
        - Use proper SQL syntax for SQLite
        
        Return your response in this STRICT JSON format:
        {{
           "thought_process": "Step-by-step reasoning for the fix...",
           "sql_query": "Corrected SQL..."
        }}
        """
    else:
        retry_prompt = full_prompt
    
    step_info = {"attempt": attempt_number, "thought": None, "sql": None, "error": None}
    
    try:
        raw_response = call_llm(retry_prompt, model_type='pro')
        parsed_response = json.loads(raw_response)
        
        step_info["thought"] = parsed_response.get("thought_process")
        step_info["sql"] = parsed_response.get("sql_query")
        
        if step_info["sql"]:
            results, columns, error = execute_query(step_info["sql"])
            if error:
                step_info["error"] = error
                should_retry = attempt_number < (max_retries + 1)
                return step_info, None, columns, should_retry
            else:
                return step_info, results, columns, False
        else:
            step_info["error"] = "No SQL generated by PRO model"
            should_retry = attempt_number < (max_retries + 1)
            return step_info, None, None, should_retry
            
    except json.JSONDecodeError:
        step_info["error"] = "Failed to parse PRO model response as JSON"
        should_retry = attempt_number < (max_retries + 1)
        return step_info, None, None, should_retry
    except Exception as e:
        step_info["error"] = f"PRO model error: {str(e)}"
        should_retry = attempt_number < (max_retries + 1)
        return step_info, None, None, should_retry


async def _generate_suggestions_async(user_question: str, sql_code: str, data_sample: list) -> list:
    """
    Async: Generate 3 follow-up question suggestions.
    """
    if not data_sample:
        return []
    
    sample_str = json.dumps(data_sample[:3], indent=2, default=str)
    
    prompt = f"""Based on this SQL query and data from a PPE violation tracking system, suggest exactly 3 short follow-up questions.
    
User's Question: {user_question}
SQL Query: {sql_code}
Sample Data: {sample_str}

Return ONLY a JSON array: ["Q1?", "Q2?", "Q3?"]
Keep questions under 10 words. Focus on violations, videos, and safety compliance."""

    try:
        raw_response = await call_llm_raw_async(prompt, model_type='flash')
        array_match = re.search(r'\[.*?\]', raw_response, re.DOTALL)
        if array_match:
            suggestions = json.loads(array_match.group(0))
            if isinstance(suggestions, list) and len(suggestions) >= 3:
                return [str(s) for s in suggestions[:3]]
        return []
    except Exception as e:
        print(f"[CHAT SUGGESTIONS] Failed: {e}")
        return []


async def _generate_data_summary_async(user_question: str, data_sample: list) -> str:
    """
    Async: Generate a 1-sentence business insight.
    """
    if not data_sample:
        return ""
    
    sample_str = json.dumps(data_sample[:5], indent=2, default=str)
    
    prompt = f"""User asked: "{user_question}"
Data found (first 5 rows): {sample_str}

Summarize the key insight in exactly 1 sentence. Be specific with numbers.
Return ONLY the summary sentence, no JSON, no quotes."""

    try:
        raw_response = await call_llm_raw_async(prompt, model_type='flash')
        summary = raw_response.strip().strip('"').strip("'")
        summary = re.sub(r'^[`\'"]+|[`\'"]+$', '', summary)
        if summary.startswith('{') or summary.startswith('['):
            return ""
        return summary
    except Exception as e:
        print(f"[CHAT SUMMARY] Failed: {e}")
        return ""


async def process_chat_question(user_question: str, previous_sql: str = None) -> dict:
    """
    Orchestrates the Text-to-SQL logic for chat queries.
    """
    response_structure = {
        "question": user_question,
        "model_used": None,
        "steps": [],
        "final_data": None,
        "columns": [],
        "suggestions": [],
        "data_summary": "",
        "status": "pending"
    }

    # 1. Router
    complexity = determine_complexity(user_question)
    response_structure["model_used"] = complexity
    
    # 2. Get schema and build prompt with current date
    schema_summary = get_schema_summary()
    current_date = datetime.now().strftime("%Y-%m-%d")
    system_prompt = build_system_prompt(schema_summary, current_date)
    
    if previous_sql:
        full_prompt = f"""{system_prompt}

### Previous Context
The user's previous query generated this SQL:
```sql
{previous_sql}
```

If the new question is a follow-up, modify the previous SQL. Otherwise, generate a fresh query.

User Question: {user_question}"""
    else:
        full_prompt = f"{system_prompt}\n\nUser Question: {user_question}"
    
    # 3. LLM Call (Attempt 1)
    step_info = {"attempt": 1, "thought": None, "sql": None, "error": None}
    needs_fallback = False
    fallback_error = None
    fallback_sql = None
    
    try:
        print(f"[CHAT] Using {complexity.upper()} model...")
        raw_response = call_llm(full_prompt, model_type=complexity)
        parsed_response = json.loads(raw_response)
        
        step_info["thought"] = parsed_response.get("thought_process")
        step_info["sql"] = parsed_response.get("sql_query")
        
        if not step_info["sql"]:
            step_info["error"] = "No SQL generated by LLM"
            needs_fallback = True
        else:
            # Execute SQL
            results, columns, error = execute_query(step_info["sql"])
            
            if error:
                step_info["error"] = error
                fallback_error = error
                fallback_sql = step_info["sql"]
                needs_fallback = True
            else:
                # Success!
                response_structure["final_data"] = results
                response_structure["columns"] = columns
                response_structure["status"] = "success"
                response_structure["steps"].append(step_info)
                
                # Generate suggestions and summary in parallel
                suggestions, summary = await asyncio.gather(
                    _generate_suggestions_async(user_question, step_info["sql"], results),
                    _generate_data_summary_async(user_question, results)
                )
                
                response_structure["suggestions"] = suggestions
                response_structure["data_summary"] = summary
                
                return response_structure
                
    except json.JSONDecodeError as e:
        step_info["error"] = f"Failed to parse LLM response as JSON: {str(e)}"
        needs_fallback = True
    except Exception as e:
        step_info["error"] = f"Unexpected error: {str(e)}"
        needs_fallback = True
    
    response_structure["steps"].append(step_info)
    
    # 4. Fallback to PRO with retries
    if needs_fallback:
        response_structure["model_used"] = "pro (fallback)"
        MAX_RETRIES = 3
        current_error = fallback_error
        current_sql = fallback_sql
        
        for attempt in range(2, MAX_RETRIES + 2):
            retry_step, results, columns, should_retry = _retry_with_pro(
                user_question=user_question,
                full_prompt=full_prompt,
                previous_error=current_error,
                previous_sql=current_sql,
                attempt_number=attempt,
                max_retries=MAX_RETRIES
            )
            
            response_structure["steps"].append(retry_step)
            
            if results is not None:
                response_structure["final_data"] = results
                response_structure["columns"] = columns
                response_structure["status"] = "success"
                
                suggestions, summary = await asyncio.gather(
                    _generate_suggestions_async(user_question, retry_step["sql"], results),
                    _generate_data_summary_async(user_question, results)
                )
                
                response_structure["suggestions"] = suggestions
                response_structure["data_summary"] = summary
                break
            
            if not should_retry:
                response_structure["status"] = "error"
                break
            
            current_error = retry_step.get("error", "Unknown error")
            current_sql = retry_step.get("sql", current_sql)
        else:
            response_structure["status"] = "error"
    
    return response_structure


async def process_chat_streaming(user_question: str, previous_sql: str = None):
    """
    Streaming version that yields SSE events progressively.
    """
    # Initial status
    yield {"event": "status", "data": "Analyzing your question..."}
    
    # Determine complexity
    complexity = determine_complexity(user_question)
    yield {"event": "model", "data": complexity}
    
    # Get schema and build prompt with current date
    yield {"event": "status", "data": "Building query..."}
    schema_summary = get_schema_summary()
    current_date = datetime.now().strftime("%Y-%m-%d")
    system_prompt = build_system_prompt(schema_summary, current_date)
    
    if previous_sql:
        full_prompt = f"""{system_prompt}

### Previous Context
Previous SQL: {previous_sql}

User Question: {user_question}"""
    else:
        full_prompt = f"{system_prompt}\n\nUser Question: {user_question}"
    
    # LLM Call
    yield {"event": "status", "data": f"Calling {complexity.upper()} model..."}
    
    step_info = {"attempt": 1, "thought": None, "sql": None, "error": None}
    needs_fallback = False
    fallback_error = None
    fallback_sql = None
    final_data = None
    columns = []
    
    try:
        raw_response = call_llm(full_prompt, model_type=complexity)
        parsed_response = json.loads(raw_response)
        
        step_info["thought"] = parsed_response.get("thought_process")
        step_info["sql"] = parsed_response.get("sql_query")
        
        if step_info["thought"]:
            yield {"event": "thought", "data": step_info["thought"]}
        
        if not step_info["sql"]:
            needs_fallback = True
            yield {"event": "status", "data": "No SQL generated, trying fallback..."}
        else:
            yield {"event": "sql", "data": step_info["sql"]}
            yield {"event": "status", "data": "Executing SQL..."}
            
            results, cols, error = execute_query(step_info["sql"])
            
            if error:
                fallback_error = error
                fallback_sql = step_info["sql"]
                needs_fallback = True
                yield {"event": "status", "data": f"SQL error, trying fallback..."}
            else:
                final_data = results
                columns = cols
                yield {"event": "table", "data": {"columns": columns, "results": [[str(v) for v in row.values()] for row in final_data]}}
                yield {"event": "status", "data": "Generating insights..."}
                
    except Exception as e:
        needs_fallback = True
        yield {"event": "status", "data": "Error occurred, trying fallback..."}
    
    # Fallback with retries
    if needs_fallback:
        MAX_RETRIES = 3
        current_error = fallback_error
        current_sql = fallback_sql
        
        for attempt in range(2, MAX_RETRIES + 2):
            yield {"event": "model", "data": f"pro (retry {attempt - 1}/{MAX_RETRIES})"}
            yield {"event": "status", "data": f"Retry {attempt - 1}/{MAX_RETRIES}..."}
            
            retry_step, results, cols, should_retry = _retry_with_pro(
                user_question=user_question,
                full_prompt=full_prompt,
                previous_error=current_error,
                previous_sql=current_sql,
                attempt_number=attempt,
                max_retries=MAX_RETRIES
            )
            
            if retry_step.get("thought"):
                yield {"event": "thought", "data": retry_step["thought"]}
            if retry_step.get("sql"):
                yield {"event": "sql", "data": retry_step["sql"]}
            
            if results is not None:
                final_data = results
                columns = cols
                yield {"event": "table", "data": {"columns": columns, "results": [[str(v) for v in row.values()] for row in final_data]}}
                yield {"event": "status", "data": "Generating insights..."}
                step_info = retry_step
                break
            
            if not should_retry:
                yield {"event": "error", "data": retry_step.get("error", "Query failed")}
                yield {"event": "done", "data": {"status": "error"}}
                return
            
            current_error = retry_step.get("error", "Unknown error")
            current_sql = retry_step.get("sql", current_sql)
        else:
            yield {"event": "error", "data": "Query failed after all retries"}
            yield {"event": "done", "data": {"status": "error"}}
            return
    
    # Post-processing
    if final_data:
        try:
            suggestions, summary = await asyncio.gather(
                _generate_suggestions_async(user_question, step_info.get("sql") or "", final_data),
                _generate_data_summary_async(user_question, final_data)
            )
            
            if suggestions:
                yield {"event": "suggestions", "data": suggestions}
            if summary:
                yield {"event": "summary", "data": summary}
                
        except Exception as e:
            print(f"[CHAT STREAMING] Post-processing error: {e}")
    
    yield {"event": "done", "data": {"status": "success"}}
